{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6226a19-3673-4017-be12-085cf5c6ed6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SVM (RBF) Feature Selection ---\n",
      "Selected features: ['Feature_asym_energy_content_below_05_Power_Spectrum_Density_AP', 'Feature_avg_fractal_dimension_ML_AND_AP', 'Feature_avg_frequency_quotient_Power_Spectrum_Density_ML', 'Feature_avg_short_time_diffusion_Diffusion_ML', 'Feature_asym_power_frequency_50_Power_Spectrum_Density_ML', 'Feature_asym_coefficient_sway_direction_ML_AND_AP', 'Feature_asym_mean_velocity_ML_AND_AP']\n",
      "=== SVM (RBF) ===\n",
      "Best params: {'clf__C': 1, 'clf__gamma': 'scale'}\n",
      "Accuracy : 0.852 ± 0.098\n",
      "Precision: 0.862 ± 0.171\n",
      "Recall   : 0.847 ± 0.051\n",
      "F1-score : 0.845 ± 0.093\n",
      "ROC-AUC  : 0.856 ± 0.110\n",
      "\n",
      "--- Random Forest Feature Selection ---\n",
      "Selected features: ['Feature_asym_energy_content_below_05_Power_Spectrum_Density_AP', 'Feature_asym_mean_value_ML', 'Feature_asym_frequency_quotient_Power_Spectrum_Density_ML', 'Feature_asym_frequency_dispersion_Power_Spectrum_Density_ML', 'Feature_asym_phase_plane_parameter_ML', 'Feature_asym_range_ML']\n",
      "=== Random Forest ===\n",
      "Best params: {'clf__max_depth': None, 'clf__n_estimators': 100}\n",
      "Accuracy : 0.843 ± 0.076\n",
      "Precision: 0.827 ± 0.071\n",
      "Recall   : 0.825 ± 0.111\n",
      "F1-score : 0.825 ± 0.086\n",
      "ROC-AUC  : 0.865 ± 0.054\n",
      "\n",
      "--- Logistic Regression Feature Selection ---\n",
      "Selected features: ['Feature_avg_fractal_dimension_ML_AND_AP', 'Feature_asym_energy_content_below_05_Power_Spectrum_Density_AP', 'Feature_avg_power_frequency_95_Power_Spectrum_Density_ML', 'Feature_asym_mean_distance_ML', 'Feature_avg_range_ratio_ML_AND_AP']\n",
      "=== Logistic Regression ===\n",
      "Best params: {'clf__C': 1}\n",
      "Accuracy : 0.835 ± 0.064\n",
      "Precision: 0.861 ± 0.139\n",
      "Recall   : 0.792 ± 0.069\n",
      "F1-score : 0.816 ± 0.065\n",
      "ROC-AUC  : 0.831 ± 0.093\n",
      "\n",
      "--- k-NN Feature Selection ---\n",
      "Selected features: ['Feature_avg_centroid_frequency_Power_Spectrum_Density_ML', 'Feature_asym_mean_distance_Radius', 'Feature_avg_mean_frequency_AP', 'Feature_asym_long_time_scaling_Diffusion_ML', 'Feature_avg_energy_content_below_05_Power_Spectrum_Density_AP', 'Feature_avg_fractal_dimension_ML_AND_AP', 'Feature_asym_energy_content_below_05_Power_Spectrum_Density_AP', 'Feature_avg_phase_plane_parameter_ML', 'Feature_avg_short_time_diffusion_Diffusion_ML']\n",
      "=== k-NN ===\n",
      "Best params: {'clf__n_neighbors': 5}\n",
      "Accuracy : 0.843 ± 0.052\n",
      "Precision: 0.849 ± 0.096\n",
      "Recall   : 0.808 ± 0.067\n",
      "F1-score : 0.825 ± 0.063\n",
      "ROC-AUC  : 0.875 ± 0.055\n",
      "\n",
      "--- Gaussian NB Feature Selection ---\n",
      "Selected features: ['Feature_avg_LFS_ML_AND_AP', 'Feature_asym_energy_content_below_05_Power_Spectrum_Density_AP', 'Feature_avg_phase_plane_parameter_ML', 'Feature_avg_fractal_dimension_ML_AND_AP', 'Feature_avg_centroid_frequency_Power_Spectrum_Density_ML', 'Feature_asym_short_time_diffusion_Diffusion_ML', 'Feature_asym_energy_content_above_2_Power_Spectrum_Density_ML', 'Feature_asym_range_AP']\n",
      "=== Gaussian NB ===\n",
      "Best params: {}\n",
      "Accuracy : 0.843 ± 0.085\n",
      "Precision: 0.811 ± 0.141\n",
      "Recall   : 0.908 ± 0.053\n",
      "F1-score : 0.848 ± 0.074\n",
      "ROC-AUC  : 0.859 ± 0.096\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.base import clone\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# --- Load data ---\n",
    "con_df = pd.read_csv(r\"E:\\USA_PD_2024\\Analysis\\ppr6\\COP\\ML\\Feature_Selection_2\\Data\\Controlled1.csv\")\n",
    "pd_df = pd.read_csv(r\"E:\\USA_PD_2024\\Analysis\\ppr6\\COP\\ML\\Feature_Selection_2\\Data\\PD1.csv\")\n",
    "\n",
    "con_df[\"label\"] = 0\n",
    "pd_df[\"label\"] = 1\n",
    "df = pd.concat([con_df, pd_df], ignore_index=True)\n",
    "\n",
    "# --- Extract SubjectID from File name ---\n",
    "def get_id(fname):\n",
    "    base = Path(fname).stem\n",
    "    m = re.search(r\"_[a-zA-Z]*([0-9]+)\", base)\n",
    "    return m.group(1) if m else base\n",
    "\n",
    "df[\"SubjectID\"] = df[\"File\"].apply(get_id)\n",
    "\n",
    "# --- Drop non-feature columns: label, File, SubjectID ---\n",
    "X = df.drop(columns=[\"label\", \"File\", \"SubjectID\"])\n",
    "y = df[\"label\"].values\n",
    "groups = df[\"SubjectID\"].values\n",
    "\n",
    "# --- Cross-validation setup ---\n",
    "cv = GroupKFold(n_splits=5)\n",
    "\n",
    "# --- Forward Selection based on F1-score ---\n",
    "def groupwise_sfs(pipe, X, y, groups):\n",
    "    best_score = 0\n",
    "    selected = []\n",
    "    remaining = list(X.columns)\n",
    "    while remaining:\n",
    "        best_feat = None\n",
    "        for feat in remaining:\n",
    "            trial_feats = selected + [feat]\n",
    "            fold_scores = []\n",
    "            for tr, te in cv.split(X[trial_feats], y, groups):\n",
    "                model = clone(pipe)\n",
    "                model.fit(X[trial_feats].iloc[tr], y[tr])\n",
    "                preds = model.predict(X[trial_feats].iloc[te])\n",
    "                fold_scores.append(f1_score(y[te], preds))\n",
    "            mean_score = np.mean(fold_scores)\n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_feat = feat\n",
    "        if best_feat:\n",
    "            selected.append(best_feat)\n",
    "            remaining.remove(best_feat)\n",
    "        else:\n",
    "            break\n",
    "    return selected\n",
    "\n",
    "# --- Define models and hyperparameters ---\n",
    "models = [\n",
    "    (\"SVM (RBF)\", Pipeline([(\"scaler\", StandardScaler()), (\"clf\", SVC(kernel=\"rbf\", probability=True, random_state=42))]),\n",
    "     {\"clf__C\": [0.1, 1, 10], \"clf__gamma\": [\"scale\", 0.1]}),\n",
    "\n",
    "    (\"Random Forest\", Pipeline([(\"clf\", RandomForestClassifier(random_state=42, n_jobs=-1))]),\n",
    "     {\"clf__n_estimators\": [100], \"clf__max_depth\": [None]}),\n",
    "\n",
    "    (\"Logistic Regression\", Pipeline([(\"scaler\", StandardScaler()), (\"clf\", LogisticRegression(max_iter=500, random_state=42))]),\n",
    "     {\"clf__C\": [1]}),\n",
    "\n",
    "    (\"k-NN\", Pipeline([(\"scaler\", StandardScaler()), (\"clf\", KNeighborsClassifier())]),\n",
    "     {\"clf__n_neighbors\": [5, 7]}),\n",
    "\n",
    "    (\"Gaussian NB\", Pipeline([(\"clf\", GaussianNB())]),\n",
    "     {})\n",
    "]\n",
    "\n",
    "# --- Train and Evaluate each model ---\n",
    "for name, pipe, param_grid in models:\n",
    "    print(f\"\\n--- {name} Feature Selection ---\")\n",
    "    selected_feats = groupwise_sfs(pipe, X, y, groups)\n",
    "    print(f\"Selected features: {selected_feats}\")\n",
    "\n",
    "    X_sel = X[selected_feats]\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    gs = GridSearchCV(pipe, param_grid, scoring=\"f1\", cv=cv, n_jobs=-1)\n",
    "    gs.fit(X_sel, y, groups=groups)\n",
    "    best_est = gs.best_estimator_\n",
    "\n",
    "    # Evaluation with best estimator\n",
    "    accs, precs, recs, f1s, aucs = [], [], [], [], []\n",
    "    for tr, te in cv.split(X_sel, y, groups):\n",
    "        best_est.fit(X_sel.iloc[tr], y[tr])\n",
    "        preds = best_est.predict(X_sel.iloc[te])\n",
    "        probs = (best_est.predict_proba(X_sel.iloc[te])[:, 1]\n",
    "                 if hasattr(best_est, \"predict_proba\")\n",
    "                 else best_est.decision_function(X_sel.iloc[te]))\n",
    "\n",
    "        accs.append(accuracy_score(y[te], preds))\n",
    "        precs.append(precision_score(y[te], preds))\n",
    "        recs.append(recall_score(y[te], preds))\n",
    "        f1s.append(f1_score(y[te], preds))\n",
    "        aucs.append(roc_auc_score(y[te], probs))\n",
    "\n",
    "    print(f\"=== {name} ===\")\n",
    "    print(f\"Best params: {gs.best_params_}\")\n",
    "    print(f\"Accuracy : {np.mean(accs):.3f} ± {np.std(accs):.3f}\")\n",
    "    print(f\"Precision: {np.mean(precs):.3f} ± {np.std(precs):.3f}\")\n",
    "    print(f\"Recall   : {np.mean(recs):.3f} ± {np.std(recs):.3f}\")\n",
    "    print(f\"F1-score : {np.mean(f1s):.3f} ± {np.std(f1s):.3f}\")\n",
    "    print(f\"ROC-AUC  : {np.mean(aucs):.3f} ± {np.std(aucs):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52dd3367-b541-461a-89c4-6da49eb50597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SVM (RBF): Feature Selection (SFS) ---\n",
      "Selected features (7): ['Feature_asym_energy_content_below_05_Power_Spectrum_Density_AP', 'Feature_avg_fractal_dimension_ML_AND_AP', 'Feature_avg_frequency_quotient_Power_Spectrum_Density_ML', 'Feature_avg_short_time_diffusion_Diffusion_ML', 'Feature_asym_power_frequency_50_Power_Spectrum_Density_ML', 'Feature_asym_coefficient_sway_direction_ML_AND_AP', 'Feature_asym_mean_velocity_ML_AND_AP']\n",
      "SVM (RBF) best params: {'clf__C': 1, 'clf__gamma': 'scale'}\n",
      "Saved permutation importance table -> feature_importance_outputs\\perm_importance_svm_rbf.csv\n",
      "Saved permutation importance figure -> feature_importance_outputs\\perm_importance_svm_rbf.png\n",
      "\n",
      "--- Random Forest: Feature Selection (SFS) ---\n",
      "Selected features (6): ['Feature_asym_energy_content_below_05_Power_Spectrum_Density_AP', 'Feature_asym_mean_value_ML', 'Feature_asym_frequency_quotient_Power_Spectrum_Density_ML', 'Feature_asym_frequency_dispersion_Power_Spectrum_Density_ML', 'Feature_asym_phase_plane_parameter_ML', 'Feature_asym_range_ML']\n",
      "Random Forest best params: {'clf__max_depth': None, 'clf__n_estimators': 100}\n",
      "Saved permutation importance table -> feature_importance_outputs\\perm_importance_random_forest.csv\n",
      "Saved permutation importance figure -> feature_importance_outputs\\perm_importance_random_forest.png\n",
      "Saved native importance (tree-based) -> feature_importance_outputs\\native_importance_random_forest.csv, feature_importance_outputs\\native_importance_random_forest.png\n",
      "\n",
      "--- Logistic Regression: Feature Selection (SFS) ---\n",
      "Selected features (5): ['Feature_avg_fractal_dimension_ML_AND_AP', 'Feature_asym_energy_content_below_05_Power_Spectrum_Density_AP', 'Feature_avg_power_frequency_95_Power_Spectrum_Density_ML', 'Feature_asym_mean_distance_ML', 'Feature_avg_range_ratio_ML_AND_AP']\n",
      "Logistic Regression best params: {'clf__C': 1}\n",
      "Saved permutation importance table -> feature_importance_outputs\\perm_importance_logistic_regression.csv\n",
      "Saved permutation importance figure -> feature_importance_outputs\\perm_importance_logistic_regression.png\n",
      "Saved coefficient-based importance -> feature_importance_outputs\\coef_importance_logistic_regression.csv, feature_importance_outputs\\coef_importance_logistic_regression.png\n",
      "\n",
      "--- k-NN: Feature Selection (SFS) ---\n",
      "Selected features (9): ['Feature_avg_centroid_frequency_Power_Spectrum_Density_ML', 'Feature_asym_mean_distance_Radius', 'Feature_avg_mean_frequency_AP', 'Feature_asym_long_time_scaling_Diffusion_ML', 'Feature_avg_energy_content_below_05_Power_Spectrum_Density_AP', 'Feature_avg_fractal_dimension_ML_AND_AP', 'Feature_asym_energy_content_below_05_Power_Spectrum_Density_AP', 'Feature_avg_phase_plane_parameter_ML', 'Feature_avg_short_time_diffusion_Diffusion_ML']\n",
      "k-NN best params: {'clf__n_neighbors': 5}\n",
      "Saved permutation importance table -> feature_importance_outputs\\perm_importance_k_nn.csv\n",
      "Saved permutation importance figure -> feature_importance_outputs\\perm_importance_k_nn.png\n",
      "\n",
      "--- Gaussian NB: Feature Selection (SFS) ---\n",
      "Selected features (8): ['Feature_avg_LFS_ML_AND_AP', 'Feature_asym_energy_content_below_05_Power_Spectrum_Density_AP', 'Feature_avg_phase_plane_parameter_ML', 'Feature_avg_fractal_dimension_ML_AND_AP', 'Feature_avg_centroid_frequency_Power_Spectrum_Density_ML', 'Feature_asym_short_time_diffusion_Diffusion_ML', 'Feature_asym_energy_content_above_2_Power_Spectrum_Density_ML', 'Feature_asym_range_AP']\n",
      "Gaussian NB best params: {}\n",
      "Saved permutation importance table -> feature_importance_outputs\\perm_importance_gaussian_nb.csv\n",
      "Saved permutation importance figure -> feature_importance_outputs\\perm_importance_gaussian_nb.png\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Redefine models\n",
    "models = [\n",
    "    (\"SVM (RBF)\", Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", SVC(kernel=\"rbf\", probability=True, random_state=42))\n",
    "    ]), {\"clf__C\": [0.1, 1, 10], \"clf__gamma\": [\"scale\", 0.1]}),\n",
    "\n",
    "    (\"Random Forest\", Pipeline([\n",
    "        (\"clf\", RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "    ]), {\"clf__n_estimators\": [100], \"clf__max_depth\": [None]}),\n",
    "\n",
    "    (\"Logistic Regression\", Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(max_iter=500, random_state=42))\n",
    "    ]), {\"clf__C\": [1]}),\n",
    "\n",
    "    (\"k-NN\", Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", KNeighborsClassifier())\n",
    "    ]), {\"clf__n_neighbors\": [5, 7]}),\n",
    "\n",
    "    (\"Gaussian NB\", Pipeline([\n",
    "        (\"clf\", GaussianNB())\n",
    "    ]), {})\n",
    "]\n",
    "# --- NEW: Imports for importance & plotting ---\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import unicodedata\n",
    "\n",
    "# --------- Helper utilities ---------\n",
    "def slugify(text):\n",
    "    \"\"\"Make safe file names from model names.\"\"\"\n",
    "    text = unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    return re.sub(r\"[^A-Za-z0-9]+\", \"_\", text).strip(\"_\").lower()\n",
    "\n",
    "def coef_importance_from_pipeline(fitted_pipe):\n",
    "    \"\"\"Return absolute coefficient importances if available, else None.\"\"\"\n",
    "    clf = fitted_pipe.named_steps.get(\"clf\", None)\n",
    "    if clf is None:\n",
    "        # Try last step if not named 'clf'\n",
    "        try:\n",
    "            clf = list(fitted_pipe.named_steps.values())[-1]\n",
    "        except Exception:\n",
    "            return None\n",
    "    if hasattr(clf, \"coef_\"):\n",
    "        coefs = np.abs(clf.coef_)\n",
    "        # Binary or multi-class: average across classes\n",
    "        if coefs.ndim == 1:\n",
    "            coefs = coefs.reshape(1, -1)\n",
    "        return coefs.mean(axis=0)\n",
    "    return None\n",
    "\n",
    "def native_importance_from_pipeline(fitted_pipe):\n",
    "    \"\"\"Return tree-based feature_importances_ if available, else None.\"\"\"\n",
    "    clf = fitted_pipe.named_steps.get(\"clf\", None)\n",
    "    if clf is None:\n",
    "        try:\n",
    "            clf = list(fitted_pipe.named_steps.values())[-1]\n",
    "        except Exception:\n",
    "            return None\n",
    "    if hasattr(clf, \"feature_importances_\"):\n",
    "        return np.asarray(clf.feature_importances_).ravel()\n",
    "    return None\n",
    "\n",
    "def plot_importance_bar(feature_names, means, stds, title, outfile, top_k=20):\n",
    "    \"\"\"Create and save a horizontal bar chart with error bars (mean ± SD).\"\"\"\n",
    "    # Sort by mean importance descending\n",
    "    order = np.argsort(means)[::-1]\n",
    "    feature_names = np.array(feature_names)[order]\n",
    "    means = np.array(means)[order]\n",
    "    stds = np.array(stds)[order]\n",
    "\n",
    "    k = min(top_k, len(feature_names))\n",
    "    fn = feature_names[:k][::-1]  # reverse for horizontal plot\n",
    "    mu = means[:k][::-1]\n",
    "    sd = stds[:k][::-1]\n",
    "\n",
    "    plt.figure(figsize=(8, max(4, 0.35 * k + 1)))\n",
    "    y_pos = np.arange(k)\n",
    "    plt.barh(y_pos, mu, xerr=sd, align='center')\n",
    "    plt.yticks(y_pos, fn)\n",
    "    plt.xlabel('Normalized importance (mean ± SD)')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outfile, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def compute_fold_importances(fitted_pipe, X_val, y_val, scoring=\"f1\", n_repeats=30, random_state=42):\n",
    "    \"\"\"\n",
    "    Compute permutation importance on held-out fold.\n",
    "    Returns (importances_mean, importances_std).\n",
    "    \"\"\"\n",
    "    r = permutation_importance(\n",
    "        fitted_pipe, X_val, y_val,\n",
    "        scoring=scoring, n_repeats=n_repeats,\n",
    "        random_state=random_state, n_jobs=-1\n",
    "    )\n",
    "    return r.importances_mean, r.importances_std\n",
    "\n",
    "# --- Directory to save outputs ---\n",
    "OUT_DIR = \"feature_importance_outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Parameters you can tweak ---\n",
    "PERM_SCORING = \"f1\"       # or \"roc_auc\"\n",
    "PERM_REPEATS = 30         # higher => smoother estimates, slower\n",
    "TOP_K_PLOT = 20           # top-k features to plot\n",
    "\n",
    "# ================== MAIN: Importance per model ==================\n",
    "for name, pipe, param_grid in models:\n",
    "    print(f\"\\n--- {name}: Feature Selection (SFS) ---\")\n",
    "    selected_feats = groupwise_sfs(pipe, X, y, groups)\n",
    "    print(f\"Selected features ({len(selected_feats)}): {selected_feats}\")\n",
    "\n",
    "    X_sel = X[selected_feats]\n",
    "\n",
    "    # Hyperparameter tuning with group-aware CV\n",
    "    gs = GridSearchCV(pipe, param_grid, scoring=\"f1\", cv=cv, n_jobs=-1, refit=True)\n",
    "    gs.fit(X_sel, y, groups=groups)\n",
    "    best_est = gs.best_estimator_\n",
    "    print(f\"{name} best params: {gs.best_params_}\")\n",
    "\n",
    "    # --- Collect fold-wise permutation importances on held-out folds ---\n",
    "    fold_perm_means = []\n",
    "    for fold_idx, (tr, te) in enumerate(cv.split(X_sel, y, groups)):\n",
    "        est = clone(best_est)\n",
    "        est.fit(X_sel.iloc[tr], y[tr])\n",
    "\n",
    "        imp_mean, _imp_std = compute_fold_importances(\n",
    "            est, X_sel.iloc[te], y[te],\n",
    "            scoring=PERM_SCORING, n_repeats=PERM_REPEATS, random_state=42 + fold_idx\n",
    "        )\n",
    "        fold_perm_means.append(imp_mean)\n",
    "\n",
    "    perm_matrix = np.vstack(fold_perm_means)  # shape: (n_folds, n_features)\n",
    "    perm_mean = perm_matrix.mean(axis=0)\n",
    "    perm_std = perm_matrix.std(axis=0)\n",
    "\n",
    "    # Normalize for readability (sum to 1)\n",
    "    total = perm_mean.sum()\n",
    "    if total > 0:\n",
    "        perm_mean_norm = perm_mean / total\n",
    "        perm_std_norm = perm_std / total\n",
    "    else:\n",
    "        perm_mean_norm = perm_mean\n",
    "        perm_std_norm = perm_std\n",
    "\n",
    "    # --- Save CSV for permutation importance ---\n",
    "    out_slug = slugify(name)\n",
    "    df_perm = pd.DataFrame({\n",
    "        \"feature\": selected_feats,\n",
    "        \"importance_mean\": perm_mean_norm,\n",
    "        \"importance_sd\": perm_std_norm\n",
    "    }).sort_values(\"importance_mean\", ascending=False)\n",
    "    csv_path = os.path.join(OUT_DIR, f\"perm_importance_{out_slug}.csv\")\n",
    "    df_perm.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved permutation importance table -> {csv_path}\")\n",
    "\n",
    "    # --- Plot permutation importance (Top-K) ---\n",
    "    png_path = os.path.join(OUT_DIR, f\"perm_importance_{out_slug}.png\")\n",
    "    plot_importance_bar(\n",
    "        df_perm[\"feature\"].tolist(),\n",
    "        df_perm[\"importance_mean\"].to_numpy(),\n",
    "        df_perm[\"importance_sd\"].to_numpy(),\n",
    "        title=f\"{name} — Permutation importance (GroupKFold, {PERM_SCORING})\",\n",
    "        outfile=png_path,\n",
    "        top_k=TOP_K_PLOT\n",
    "    )\n",
    "    print(f\"Saved permutation importance figure -> {png_path}\")\n",
    "\n",
    "    # --- OPTIONAL: also save native importances if available (coeffs or feature_importances_) ---\n",
    "    # Fit once on all data (with selected features) ONLY to extract native importances.\n",
    "    # (Use with caution in text; for figures, prefer the CV-based permutation results above.)\n",
    "    est_full = clone(best_est).fit(X_sel, y)\n",
    "    native_imp = native_importance_from_pipeline(est_full)\n",
    "    coef_imp = coef_importance_from_pipeline(est_full)\n",
    "\n",
    "    if native_imp is not None:\n",
    "        # Normalize\n",
    "        n_total = native_imp.sum()\n",
    "        n_imp = native_imp / n_total if n_total > 0 else native_imp\n",
    "        df_nat = pd.DataFrame({\"feature\": selected_feats, \"importance\": n_imp}) \\\n",
    "                    .sort_values(\"importance\", ascending=False)\n",
    "        csv_nat = os.path.join(OUT_DIR, f\"native_importance_{out_slug}.csv\")\n",
    "        df_nat.to_csv(csv_nat, index=False)\n",
    "        png_nat = os.path.join(OUT_DIR, f\"native_importance_{out_slug}.png\")\n",
    "        plot_importance_bar(\n",
    "            df_nat[\"feature\"].tolist(),\n",
    "            df_nat[\"importance\"].to_numpy(),\n",
    "            np.zeros_like(df_nat[\"importance\"].to_numpy()),\n",
    "            title=f\"{name} — Native importance (fit on all data)\",\n",
    "            outfile=png_nat,\n",
    "            top_k=TOP_K_PLOT\n",
    "        )\n",
    "        print(f\"Saved native importance (tree-based) -> {csv_nat}, {png_nat}\")\n",
    "\n",
    "    if coef_imp is not None:\n",
    "        # Normalize absolute coefficients\n",
    "        c_total = coef_imp.sum()\n",
    "        c_imp = coef_imp / c_total if c_total > 0 else coef_imp\n",
    "        df_coef = pd.DataFrame({\"feature\": selected_feats, \"importance\": c_imp}) \\\n",
    "                    .sort_values(\"importance\", ascending=False)\n",
    "        csv_coef = os.path.join(OUT_DIR, f\"coef_importance_{out_slug}.csv\")\n",
    "        df_coef.to_csv(csv_coef, index=False)\n",
    "        png_coef = os.path.join(OUT_DIR, f\"coef_importance_{out_slug}.png\")\n",
    "        plot_importance_bar(\n",
    "            df_coef[\"feature\"].tolist(),\n",
    "            df_coef[\"importance\"].to_numpy(),\n",
    "            np.zeros_like(df_coef[\"importance\"].to_numpy()),\n",
    "            title=f\"{name} — |Coefficient| importance (fit on all data)\",\n",
    "            outfile=png_coef,\n",
    "            top_k=TOP_K_PLOT\n",
    "        )\n",
    "        print(f\"Saved coefficient-based importance -> {csv_coef}, {png_coef}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e092754-13af-416e-9177-1d18d42b3d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All selected features are present.\n",
      "⚠️ 'Sex' column not found. Proceeding without it.\n",
      "\n",
      "🔹 SVM (RBF) Model 🔹\n",
      "Best Params: {'clf__C': 1, 'clf__gamma': 'scale'}\n",
      "Accuracy : 0.852 ± 0.098\n",
      "Precision: 0.862 ± 0.171\n",
      "Recall   : 0.847 ± 0.051\n",
      "F1-score : 0.845 ± 0.093\n",
      "ROC-AUC  : 0.856 ± 0.110\n",
      "\n",
      "🔹 Random Forest Model 🔹\n",
      "Best Params: {'clf__max_depth': None, 'clf__n_estimators': 100}\n",
      "Accuracy : 0.739 ± 0.106\n",
      "Precision: 0.712 ± 0.141\n",
      "Recall   : 0.770 ± 0.086\n",
      "F1-score : 0.732 ± 0.100\n",
      "ROC-AUC  : 0.788 ± 0.132\n",
      "\n",
      "🔹 Logistic Regression Model 🔹\n",
      "Best Params: {'clf__C': 1}\n",
      "Accuracy : 0.678 ± 0.115\n",
      "Precision: 0.676 ± 0.151\n",
      "Recall   : 0.695 ± 0.103\n",
      "F1-score : 0.669 ± 0.090\n",
      "ROC-AUC  : 0.786 ± 0.118\n",
      "\n",
      "🔹 k-NN Model 🔹\n",
      "Best Params: {'clf__n_neighbors': 7}\n",
      "Accuracy : 0.783 ± 0.091\n",
      "Precision: 0.813 ± 0.159\n",
      "Recall   : 0.737 ± 0.089\n",
      "F1-score : 0.761 ± 0.083\n",
      "ROC-AUC  : 0.835 ± 0.122\n",
      "\n",
      "🔹 Gaussian NB Model 🔹\n",
      "Best Params: {}\n",
      "Accuracy : 0.748 ± 0.093\n",
      "Precision: 0.758 ± 0.170\n",
      "Recall   : 0.715 ± 0.066\n",
      "F1-score : 0.727 ± 0.093\n",
      "ROC-AUC  : 0.829 ± 0.116\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Binary PD vs Control classification using manually selected CoP + demographic features\n",
    "──────────────────────────────────────────────────────────────────────────────────────\n",
    "Features: 15 CoP-derived + Height, Weight, Sex (if available)\n",
    "Modeling: SVM, RF, LR, k-NN, GNB\n",
    "Validation: 5-fold GroupKFold using SubjectID\n",
    "\"\"\"\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 1. Imports\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.base import clone\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 2. Load and concatenate data\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "con_df = pd.read_csv(r\"E:\\USA_PD_2024\\Analysis\\ppr6\\COP\\ML\\Feature_Selection_2\\Data\\Controlled1.csv\")\n",
    "pd_df  = pd.read_csv(r\"E:\\USA_PD_2024\\Analysis\\ppr6\\COP\\ML\\Feature_Selection_2\\Data\\PD1.csv\")\n",
    "\n",
    "con_df[\"label\"] = 0\n",
    "pd_df [\"label\"] = 1\n",
    "df = pd.concat([con_df, pd_df], ignore_index=True)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 3. Clean column names\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "df.columns = df.columns.str.strip().str.replace(\" \", \"_\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 4. Extract Subject ID from filename\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "def get_id(fname: str) -> str:\n",
    "    base = Path(fname).stem\n",
    "    m = re.search(r\"_[a-zA-Z]*([0-9]+)\", base)\n",
    "    return m.group(1) if m else base\n",
    "\n",
    "df[\"SubjectID\"] = df[\"File\"].apply(get_id)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 5. Manually selected features\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "manual_features = [\n",
    "    \"Feature_asym_energy_content_below_05_Power_Spectrum_Density_AP\",\n",
    "    \"Feature_avg_fractal_dimension_ML_AND_AP\",\n",
    "    \"Feature_avg_frequency_quotient_Power_Spectrum_Density_ML\",\n",
    "    \"Feature_avg_short_time_diffusion_Diffusion_ML\",\n",
    "    \"Feature_asym_power_frequency_50_Power_Spectrum_Density_ML\",\n",
    "    \"Feature_asym_coefficient_sway_direction_ML_AND_AP\",\n",
    "    \"Feature_asym_mean_velocity_ML_AND_AP\"\n",
    "]\n",
    "\n",
    "# Check availability\n",
    "available_features = [f for f in manual_features if f in df.columns]\n",
    "missing = set(manual_features) - set(available_features)\n",
    "if missing:\n",
    "    print(f\"⚠️ Warning: Missing features skipped: {missing}\")\n",
    "else:\n",
    "    print(\"✅ All selected features are present.\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 6. Prepare feature matrix X\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "if \"Sex\" in df.columns:\n",
    "    X = df[available_features + [\"Sex\"]].copy()\n",
    "    X = pd.get_dummies(X, columns=[\"Sex\"], drop_first=True)\n",
    "else:\n",
    "    X = df[available_features].copy()\n",
    "    print(\"⚠️ 'Sex' column not found. Proceeding without it.\")\n",
    "\n",
    "y = df[\"label\"].values\n",
    "groups = df[\"SubjectID\"].values\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 7. Cross-validation setup\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "cv = GroupKFold(n_splits=5)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 8. Models and parameter grids\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "models = [\n",
    "    (\"SVM (RBF)\", Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", SVC(kernel=\"rbf\", probability=True, random_state=42))\n",
    "    ]), {\"clf__C\": [0.1, 1, 10], \"clf__gamma\": [\"scale\", 0.1]}),\n",
    "\n",
    "    (\"Random Forest\", Pipeline([\n",
    "        (\"clf\", RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "    ]), {\"clf__n_estimators\": [100], \"clf__max_depth\": [None]}),\n",
    "\n",
    "    (\"Logistic Regression\", Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", LogisticRegression(max_iter=500, random_state=42))\n",
    "    ]), {\"clf__C\": [1]}),\n",
    "\n",
    "    (\"k-NN\", Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"clf\", KNeighborsClassifier())\n",
    "    ]), {\"clf__n_neighbors\": [5, 7]}),\n",
    "\n",
    "    (\"Gaussian NB\", Pipeline([\n",
    "        (\"clf\", GaussianNB())\n",
    "    ]), {})\n",
    "]\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 9. Train, tune and evaluate\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "for name, pipe, param_grid in models:\n",
    "    print(f\"\\n🔹 {name} Model 🔹\")\n",
    "    \n",
    "    # Grid Search CV\n",
    "    gs = GridSearchCV(pipe, param_grid, scoring=\"f1\", cv=cv, n_jobs=-1)\n",
    "    gs.fit(X, y, groups=groups)\n",
    "    best_est = gs.best_estimator_\n",
    "\n",
    "    # Evaluate with group-wise CV\n",
    "    accs, precs, recs, f1s, aucs = [], [], [], [], []\n",
    "    for tr, te in cv.split(X, y, groups):\n",
    "        best_est.fit(X.iloc[tr], y[tr])\n",
    "        preds = best_est.predict(X.iloc[te])\n",
    "        probs = (best_est.predict_proba(X.iloc[te])[:, 1]\n",
    "                 if hasattr(best_est, \"predict_proba\")\n",
    "                 else best_est.decision_function(X.iloc[te]))\n",
    "\n",
    "        accs.append(accuracy_score(y[te], preds))\n",
    "        precs.append(precision_score(y[te], preds, zero_division=0))\n",
    "        recs.append(recall_score(y[te], preds, zero_division=0))\n",
    "        f1s.append(f1_score(y[te], preds, zero_division=0))\n",
    "        aucs.append(roc_auc_score(y[te], probs))\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Best Params: {gs.best_params_}\")\n",
    "    print(f\"Accuracy : {np.mean(accs):.3f} ± {np.std(accs):.3f}\")\n",
    "    print(f\"Precision: {np.mean(precs):.3f} ± {np.std(precs):.3f}\")\n",
    "    print(f\"Recall   : {np.mean(recs):.3f} ± {np.std(recs):.3f}\")\n",
    "    print(f\"F1-score : {np.mean(f1s):.3f} ± {np.std(f1s):.3f}\")\n",
    "    print(f\"ROC-AUC  : {np.mean(aucs):.3f} ± {np.std(aucs):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7681a9df-8f54-4508-a896-437e90c6a835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
